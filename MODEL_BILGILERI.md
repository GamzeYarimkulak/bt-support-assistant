# KullanÄ±lan Modeller - DetaylÄ± Bilgiler

## ğŸ“Š Embedding Model (Anlam TabanlÄ± Arama)

### Model: `sentence-transformers/all-MiniLM-L6-v2`

**Kaynak:** Hugging Face  
**GeliÅŸtirici:** Microsoft  
**Model Tipi:** Sentence Transformer (Encoder-only)

#### Teknik Ã–zellikler:
- **Parametre SayÄ±sÄ±:** ~22.7M (22.7 milyon parametre)
- **Embedding Boyutu:** 384 boyut
- **Maksimum Sequence Length:** 256 token
- **Model Boyutu:** ~80 MB (indirildiÄŸinde)
- **Dil DesteÄŸi:** Ã‡ok dilli (100+ dil, TÃ¼rkÃ§e dahil)

#### Performans:
- **HÄ±z:** Ã‡ok hÄ±zlÄ± (CPU'da ~1000 cÃ¼mle/saniye)
- **Bellek:** DÃ¼ÅŸÃ¼k (RAM'de ~200 MB)
- **DoÄŸruluk:** Orta-iyi seviye (daha bÃ¼yÃ¼k modellere gÃ¶re)

#### KullanÄ±m AmacÄ±:
- DokÃ¼man ve sorgularÄ± 384 boyutlu vektÃ¶rlere dÃ¶nÃ¼ÅŸtÃ¼rme
- Semantic (anlam) benzerliÄŸi hesaplama
- FAISS ile hÄ±zlÄ± benzerlik aramasÄ±

#### Alternatif Modeller (Daha Ä°yi Performans Ä°Ã§in):
1. **`sentence-transformers/all-mpnet-base-v2`**
   - 110M parametre, 768 boyut
   - Daha iyi doÄŸruluk, daha yavaÅŸ
   
2. **`intfloat/multilingual-e5-base`**
   - Ã‡ok dilli odaklÄ±
   - TÃ¼rkÃ§e iÃ§in daha iyi performans

3. **`paraphrase-multilingual-MiniLM-L12-v2`**
   - Ã‡ok dilli, 384 boyut
   - TÃ¼rkÃ§e iÃ§in optimize edilmiÅŸ

---

## ğŸ¤– LLM Model (YanÄ±t Ãœretimi)

### Åu Anki Durum: Ä°ki SeÃ§enek Var

#### 1. OpenAI API (Åu An Aktif - PHASE 8)
**Model:** `gpt-4o-mini` (varsayÄ±lan)

**Teknik Ã–zellikler:**
- **Parametre SayÄ±sÄ±:** AÃ§Ä±klanmamÄ±ÅŸ (tahmin: ~1-2B)
- **Context Length:** 128K token
- **Maksimum Output:** 16K token
- **Dil DesteÄŸi:** Ã‡ok dilli (TÃ¼rkÃ§e dahil)
- **Maliyet:** DÃ¼ÅŸÃ¼k (gpt-4o'ya gÃ¶re %10-20)

**Alternatif OpenAI Modelleri:**
- `gpt-4o`: Daha gÃ¼Ã§lÃ¼, daha pahalÄ±
- `gpt-4-turbo`: En gÃ¼Ã§lÃ¼, en pahalÄ±
- `gpt-3.5-turbo`: Daha ucuz, daha zayÄ±f

#### 2. Yerel Model (Yedek - HenÃ¼z Aktif DeÄŸil)
**Model:** `mistralai/Mistral-7B-Instruct-v0.2`

**Teknik Ã–zellikler:**
- **Parametre SayÄ±sÄ±:** 7.24B (7.24 milyar parametre)
- **Context Length:** 32K token
- **Model Boyutu:** ~14 GB (quantized: ~4-7 GB)
- **Dil DesteÄŸi:** Ã‡ok dilli (TÃ¼rkÃ§e dahil)
- **Lisans:** Apache 2.0 (aÃ§Ä±k kaynak)

**Gereksinimler:**
- GPU: En az 8GB VRAM (16GB Ã¶nerilir)
- RAM: En az 16GB
- Disk: ~14GB (full precision) veya ~7GB (quantized)

**KullanÄ±m Durumu:**
- Åu anda `USE_REAL_LLM=false` olduÄŸu iÃ§in kullanÄ±lmÄ±yor
- Stub (sahte) yanÄ±t Ã¼reticisi kullanÄ±lÄ±yor
- OpenAI API key varsa gerÃ§ek LLM kullanÄ±labilir

---

## ğŸ“ˆ Model KarÅŸÄ±laÅŸtÄ±rmasÄ±

| Ã–zellik | Embedding (MiniLM) | LLM (gpt-4o-mini) | LLM (Mistral-7B) |
|---------|-------------------|-------------------|-------------------|
| **Parametre** | 22.7M | ~1-2B (tahmin) | 7.24B |
| **Boyut** | 80 MB | API (yok) | 14 GB |
| **HÄ±z** | Ã‡ok HÄ±zlÄ± | Orta | YavaÅŸ (yerel) |
| **Maliyet** | Ãœcretsiz | DÃ¼ÅŸÃ¼k ($) | Ãœcretsiz |
| **Gizlilik** | Yerel | Bulut | Yerel |
| **TÃ¼rkÃ§e** | Ä°yi | MÃ¼kemmel | Ä°yi |

---

## ğŸ”§ Model AyarlarÄ± (.env dosyasÄ±nda)

```env
# Embedding Model
EMBEDDING_MODEL_NAME=sentence-transformers/all-MiniLM-L6-v2

# LLM Model (OpenAI)
USE_REAL_LLM=true
LLM_MODEL=gpt-4o-mini
OPENAI_API_KEY=sk-...

# LLM Model (Yerel - henÃ¼z kullanÄ±lmÄ±yor)
LLM_MODEL_NAME=mistralai/Mistral-7B-Instruct-v0.2
```

---

## ğŸ’¡ Ã–neriler

### Embedding Model Ä°Ã§in:
- **Mevcut model yeterli** - HÄ±zlÄ± ve verimli
- TÃ¼rkÃ§e performansÄ± artÄ±rmak iÃ§in `paraphrase-multilingual-MiniLM-L12-v2` denenebilir
- Daha iyi doÄŸruluk iÃ§in `all-mpnet-base-v2` kullanÄ±labilir (daha yavaÅŸ)

### LLM Model Ä°Ã§in:
- **Åu an iÃ§in OpenAI API kullanÄ±mÄ± Ã¶nerilir** - Kolay ve etkili
- Yerel model sadece veri gizliliÄŸi kritikse gerekli
- Mistral-7B yerel kullanÄ±m iÃ§in iyi bir seÃ§enek

---

## ğŸ“š Kaynaklar

- **Embedding Model:** https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2
- **Mistral-7B:** https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2
- **OpenAI Models:** https://platform.openai.com/docs/models



















