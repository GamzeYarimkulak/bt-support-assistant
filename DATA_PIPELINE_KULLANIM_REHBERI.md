# Data Pipeline KullanÄ±m Rehberi

Bu rehber, yeni eklenen data pipeline script'lerinin nasÄ±l kullanÄ±lacaÄŸÄ±nÄ± aÃ§Ä±klar.

---

## ğŸ“ Dizin YapÄ±sÄ±

Ã–nce standart dizin yapÄ±sÄ±nÄ± oluÅŸturun:

```bash
python scripts/init_data_dirs.py
```

Bu komut ÅŸu dizinleri oluÅŸturur:
- `data/raw/tickets/` - CSV ticket dosyalarÄ± buraya
- `data/raw/kb/` - PDF KB dosyalarÄ± buraya
- `data/processed/` - Ä°ÅŸlenmiÅŸ dosyalar buraya yazÄ±lÄ±r

**Not:** Mevcut CSV/PDF dosyalarÄ±nÄ±zÄ± elle taÅŸÄ±manÄ±z gerekir. Script otomatik taÅŸÄ±maz.

---

## 1ï¸âƒ£ Ticket Ingestion (CSV â†’ Parquet)

CSV ticket dosyalarÄ±nÄ± standart parquet formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.

### KullanÄ±m

```bash
# Temel kullanÄ±m
python data_pipeline/ingest_tickets.py

# Ã–zel input/output
python data_pipeline/ingest_tickets.py --input-dir data/raw/tickets --output data/processed/tickets.parquet

# Dry-run (sadece analiz, dosya yazmaz)
python data_pipeline/ingest_tickets.py --dry-run

# Test iÃ§in limit
python data_pipeline/ingest_tickets.py --limit 100
```

### Ã‡Ä±ktÄ±

- `data/processed/tickets.parquet` - Standart ÅŸemalÄ± parquet dosyasÄ±

### Åema

- `id`: str
- `text`: str (subject + body birleÅŸtirilmiÅŸ)
- `resolution`: str
- `category`: str
- `priority`: str
- `language`: str
- `created_at`: datetime veya None
- `source`: str (kaynak dosya adÄ±)

### Ã–zellikler

- Otomatik kolon mapping (farklÄ± CSV formatlarÄ±nÄ± destekler)
- KVKK uyumu iÃ§in anonimleÅŸtirme (settings.anonymization_enabled kontrol eder)
- DetaylÄ± log ve rapor

---

## 2ï¸âƒ£ KB Ingestion (PDF â†’ JSONL)

PDF dosyalarÄ±nÄ± chunk'lara bÃ¶lerek JSONL formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.

### KullanÄ±m

```bash
# Temel kullanÄ±m
python data_pipeline/ingest_kb.py

# Ã–zel input/output
python data_pipeline/ingest_kb.py --input-dir data/raw/kb --output data/processed/kb_chunks.jsonl

# Dry-run
python data_pipeline/ingest_kb.py --dry-run

# Test iÃ§in sayfa limiti
python data_pipeline/ingest_kb.py --max-pages 10

# Chunk boyutu ayarlama
python data_pipeline/ingest_kb.py --chunk-size 500
```

### Ã‡Ä±ktÄ±

- `data/processed/kb_chunks.jsonl` - Her satÄ±r bir chunk (JSON formatÄ±nda)

### Åema (her chunk)

- `id`: str (unique chunk ID)
- `text`: str (chunk metni)
- `source_pdf`: str (kaynak PDF dosya adÄ±)
- `page`: int (sayfa numarasÄ±)
- `chunk_index`: int (sayfa iÃ§indeki chunk indeksi)

### Ã–zellikler

- ~400 token chunk boyutu (ayarlanabilir)
- Sayfa bazlÄ± chunking
- Overlap desteÄŸi (chunk'lar arasÄ± geÃ§iÅŸ)

### Gereksinimler

```bash
pip install pypdf
```

---

## 3ï¸âƒ£ Index Build (Parquet + JSONL â†’ Indexes)

Ä°ÅŸlenmiÅŸ verilerden BM25 ve FAISS index'lerini oluÅŸturur.

### KullanÄ±m

```bash
# Temel kullanÄ±m
python scripts/build_indexes.py

# Ã–zel dosya yollarÄ±
python scripts/build_indexes.py --tickets data/processed/tickets.parquet --kb data/processed/kb_chunks.jsonl

# Dry-run
python scripts/build_indexes.py --dry-run

# Test iÃ§in limit
python scripts/build_indexes.py --limit 1000

# Mevcut index'leri yeniden oluÅŸtur
python scripts/build_indexes.py --rebuild
```

### Ã‡Ä±ktÄ±

- `indexes/bm25_index.pkl` - BM25 index
- `indexes/faiss_index.bin` - FAISS index
- `indexes/embedding_data.pkl` - Embedding data
- `indexes/index_metadata.json` - Metadata

### Ã–zellikler

- Mevcut index'leri korur (--rebuild ile yeniden oluÅŸturulabilir)
- Ticket ve KB chunk'larÄ±nÄ± birleÅŸtirir
- Mevcut IndexBuilder sÄ±nÄ±fÄ±nÄ± kullanÄ±r (mevcut kodu bozmaz)

---

## 4ï¸âƒ£ Retrieval Evaluation

Retrieval performansÄ±nÄ± Ã¶lÃ§er (Recall@5, nDCG@10, latency).

### KullanÄ±m

```bash
# Temel kullanÄ±m
python scripts/evaluate_retrieval.py

# Query sayÄ±sÄ± ayarlama
python scripts/evaluate_retrieval.py --n-queries 200

# Random seed ayarlama
python scripts/evaluate_retrieval.py --seed 123

# Ã–zel output dosyasÄ±
python scripts/evaluate_retrieval.py --output my_results.json
```

### Ã‡Ä±ktÄ±

- `test_results.json` - Evaluation sonuÃ§larÄ± (JSON formatÄ±nda)

### Metrikler

- **Recall@5**: Ä°lk 5 sonuÃ§ta ground truth bulunma oranÄ±
- **nDCG@10**: Normalized Discounted Cumulative Gain @ 10
- **Average Latency**: Ortalama sorgu sÃ¼resi (saniye)

### Ground Truth

Basit yaklaÅŸÄ±m: Ticket'Ä±n `text` alanÄ± sorgu, `resolution` alanÄ± ground truth olarak kullanÄ±lÄ±r.

---

## ğŸ”„ Tam Pipeline Ã–rneÄŸi

```bash
# 1. Dizinleri oluÅŸtur
python scripts/init_data_dirs.py

# 2. CSV dosyalarÄ±nÄ± data/raw/tickets/ klasÃ¶rÃ¼ne taÅŸÄ± (elle)

# 3. PDF dosyalarÄ±nÄ± data/raw/kb/ klasÃ¶rÃ¼ne taÅŸÄ± (elle)

# 4. Ticket'larÄ± iÅŸle
python data_pipeline/ingest_tickets.py

# 5. KB dosyalarÄ±nÄ± iÅŸle
python data_pipeline/ingest_kb.py

# 6. Index'leri oluÅŸtur
python scripts/build_indexes.py

# 7. PerformansÄ± Ã¶lÃ§
python scripts/evaluate_retrieval.py
```

---

## âš™ï¸ Ayarlar

TÃ¼m script'ler `app/config.py` iÃ§indeki settings'i kullanÄ±r:

- `settings.data_dir` - Veri dizini (default: "./data")
- `settings.anonymization_enabled` - AnonimleÅŸtirme aÃ§Ä±k/kapalÄ± (default: True)
- `settings.embedding_model_name` - Embedding model (default: "sentence-transformers/all-MiniLM-L6-v2")

---

## ğŸ› Sorun Giderme

### "pypdf not installed" hatasÄ±

```bash
pip install pypdf
```

### "Parquet file not found" hatasÄ±

Ã–nce `ingest_tickets.py` Ã§alÄ±ÅŸtÄ±rÄ±n.

### "Index files already exist" hatasÄ±

`--rebuild` flag'i ile yeniden oluÅŸturun:
```bash
python scripts/build_indexes.py --rebuild
```

### AnonimleÅŸtirme Ã§alÄ±ÅŸmÄ±yor

`data_pipeline/anonymize.py` dosyasÄ±nÄ±n mevcut olduÄŸundan emin olun. Settings'te `anonymization_enabled=True` olmalÄ±.

---

## ğŸ“ Notlar

- TÃ¼m script'ler `--dry-run` desteÄŸi ile gÃ¼venli test edilebilir
- Mevcut kod hiÃ§bir ÅŸekilde deÄŸiÅŸtirilmedi, sadece yeni dosyalar eklendi
- Settings'ten path okunur, hardcode path yok
- Unit testler bozulmadÄ±








